WARNING: Logging before InitGoogleLogging() is written to STDERR
I0814 21:51:56.419517 13520 solver.cpp:48] Initializing solver from parameters: 
test_iter: 172
test_interval: 100
base_lr: 1e-05
display: 100
max_iter: 40000
lr_policy: "inv"
gamma: 5e-05
power: 0.75
momentum: 0.9
weight_decay: 2e-05
snapshot: 5000
snapshot_prefix: "models/person_background_only_mirror/person_background_only_lr_0.00001"
solver_mode: GPU
net: "nets/person_background_only_mirror/trainval.prototxt"
I0814 21:51:56.419648 13520 solver.cpp:91] Creating training net from net file: nets/person_background_only_mirror/trainval.prototxt
I0814 21:51:56.419833 13520 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0814 21:51:56.419849 13520 net.cpp:322] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0814 21:51:56.419926 13520 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mirror: true
    mean_file: "data/person_only_lmdb/person_background_only_color_mean.binaryproto"
  }
  data_param {
    source: "data/person_only_lmdb/person_background_only_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0814 21:51:56.419970 13520 layer_factory.hpp:77] Creating layer mnist
I0814 21:51:56.421975 13520 net.cpp:100] Creating Layer mnist
I0814 21:51:56.421993 13520 net.cpp:408] mnist -> data
I0814 21:51:56.422005 13520 net.cpp:408] mnist -> label
I0814 21:51:56.422027 13520 data_transformer.cpp:25] Loading mean file from: data/person_only_lmdb/person_background_only_color_mean.binaryproto
I0814 21:51:56.423570 13530 db_lmdb.cpp:35] Opened lmdb data/person_only_lmdb/person_background_only_train_lmdb
I0814 21:51:56.456853 13520 data_layer.cpp:41] output data size: 64,3,128,128
I0814 21:51:56.487052 13520 net.cpp:150] Setting up mnist
I0814 21:51:56.487092 13520 net.cpp:157] Top shape: 64 3 128 128 (3145728)
I0814 21:51:56.487098 13520 net.cpp:157] Top shape: 64 (64)
I0814 21:51:56.487102 13520 net.cpp:165] Memory required for data: 12583168
I0814 21:51:56.487110 13520 layer_factory.hpp:77] Creating layer conv1
I0814 21:51:56.487138 13520 net.cpp:100] Creating Layer conv1
I0814 21:51:56.487144 13520 net.cpp:434] conv1 <- data
I0814 21:51:56.487156 13520 net.cpp:408] conv1 -> conv1
I0814 21:51:56.789834 13520 net.cpp:150] Setting up conv1
I0814 21:51:56.789872 13520 net.cpp:157] Top shape: 64 96 124 124 (94470144)
I0814 21:51:56.789877 13520 net.cpp:165] Memory required for data: 390463744
I0814 21:51:56.789894 13520 layer_factory.hpp:77] Creating layer pool1
I0814 21:51:56.789909 13520 net.cpp:100] Creating Layer pool1
I0814 21:51:56.789913 13520 net.cpp:434] pool1 <- conv1
I0814 21:51:56.789919 13520 net.cpp:408] pool1 -> pool1
I0814 21:51:56.789970 13520 net.cpp:150] Setting up pool1
I0814 21:51:56.789984 13520 net.cpp:157] Top shape: 64 96 62 62 (23617536)
I0814 21:51:56.789988 13520 net.cpp:165] Memory required for data: 484933888
I0814 21:51:56.789989 13520 layer_factory.hpp:77] Creating layer conv2
I0814 21:51:56.790002 13520 net.cpp:100] Creating Layer conv2
I0814 21:51:56.790005 13520 net.cpp:434] conv2 <- pool1
I0814 21:51:56.790010 13520 net.cpp:408] conv2 -> conv2
I0814 21:51:56.794188 13520 net.cpp:150] Setting up conv2
I0814 21:51:56.794204 13520 net.cpp:157] Top shape: 64 50 58 58 (10764800)
I0814 21:51:56.794209 13520 net.cpp:165] Memory required for data: 527993088
I0814 21:51:56.794217 13520 layer_factory.hpp:77] Creating layer pool2
I0814 21:51:56.794225 13520 net.cpp:100] Creating Layer pool2
I0814 21:51:56.794229 13520 net.cpp:434] pool2 <- conv2
I0814 21:51:56.794234 13520 net.cpp:408] pool2 -> pool2
I0814 21:51:56.794277 13520 net.cpp:150] Setting up pool2
I0814 21:51:56.794286 13520 net.cpp:157] Top shape: 64 50 29 29 (2691200)
I0814 21:51:56.794288 13520 net.cpp:165] Memory required for data: 538757888
I0814 21:51:56.794291 13520 layer_factory.hpp:77] Creating layer ip1
I0814 21:51:56.794301 13520 net.cpp:100] Creating Layer ip1
I0814 21:51:56.794304 13520 net.cpp:434] ip1 <- pool2
I0814 21:51:56.794311 13520 net.cpp:408] ip1 -> ip1
I0814 21:51:56.974370 13520 net.cpp:150] Setting up ip1
I0814 21:51:56.974409 13520 net.cpp:157] Top shape: 64 500 (32000)
I0814 21:51:56.974413 13520 net.cpp:165] Memory required for data: 538885888
I0814 21:51:56.974431 13520 layer_factory.hpp:77] Creating layer relu1
I0814 21:51:56.974445 13520 net.cpp:100] Creating Layer relu1
I0814 21:51:56.974449 13520 net.cpp:434] relu1 <- ip1
I0814 21:51:56.974457 13520 net.cpp:395] relu1 -> ip1 (in-place)
I0814 21:51:56.974716 13520 net.cpp:150] Setting up relu1
I0814 21:51:56.974727 13520 net.cpp:157] Top shape: 64 500 (32000)
I0814 21:51:56.974730 13520 net.cpp:165] Memory required for data: 539013888
I0814 21:51:56.974732 13520 layer_factory.hpp:77] Creating layer ip2
I0814 21:51:56.974742 13520 net.cpp:100] Creating Layer ip2
I0814 21:51:56.974745 13520 net.cpp:434] ip2 <- ip1
I0814 21:51:56.974751 13520 net.cpp:408] ip2 -> ip2
I0814 21:51:56.974879 13520 net.cpp:150] Setting up ip2
I0814 21:51:56.974887 13520 net.cpp:157] Top shape: 64 2 (128)
I0814 21:51:56.974890 13520 net.cpp:165] Memory required for data: 539014400
I0814 21:51:56.974895 13520 layer_factory.hpp:77] Creating layer loss
I0814 21:51:56.974905 13520 net.cpp:100] Creating Layer loss
I0814 21:51:56.974907 13520 net.cpp:434] loss <- ip2
I0814 21:51:56.974911 13520 net.cpp:434] loss <- label
I0814 21:51:56.974916 13520 net.cpp:408] loss -> loss
I0814 21:51:56.974926 13520 layer_factory.hpp:77] Creating layer loss
I0814 21:51:56.975512 13520 net.cpp:150] Setting up loss
I0814 21:51:56.975528 13520 net.cpp:157] Top shape: (1)
I0814 21:51:56.975530 13520 net.cpp:160]     with loss weight 1
I0814 21:51:56.975541 13520 net.cpp:165] Memory required for data: 539014404
I0814 21:51:56.975545 13520 net.cpp:226] loss needs backward computation.
I0814 21:51:56.975550 13520 net.cpp:226] ip2 needs backward computation.
I0814 21:51:56.975554 13520 net.cpp:226] relu1 needs backward computation.
I0814 21:51:56.975558 13520 net.cpp:226] ip1 needs backward computation.
I0814 21:51:56.975560 13520 net.cpp:226] pool2 needs backward computation.
I0814 21:51:56.975564 13520 net.cpp:226] conv2 needs backward computation.
I0814 21:51:56.975567 13520 net.cpp:226] pool1 needs backward computation.
I0814 21:51:56.975570 13520 net.cpp:226] conv1 needs backward computation.
I0814 21:51:56.975574 13520 net.cpp:228] mnist does not need backward computation.
I0814 21:51:56.975576 13520 net.cpp:270] This network produces output loss
I0814 21:51:56.975586 13520 net.cpp:283] Network initialization done.
I0814 21:51:56.975806 13520 solver.cpp:181] Creating test net (#0) specified by net file: nets/person_background_only_mirror/trainval.prototxt
I0814 21:51:56.975836 13520 net.cpp:322] The NetState phase (1) differed from the phase (0) specified by a rule in layer mnist
I0814 21:51:56.975942 13520 net.cpp:58] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TEST
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_file: "data/person_only_lmdb/person_background_only_color_mean.binaryproto"
  }
  data_param {
    source: "data/person_only_lmdb/person_background_only_test_lmdb"
    batch_size: 100
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 96
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "ip2"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0814 21:51:56.976012 13520 layer_factory.hpp:77] Creating layer mnist
I0814 21:51:56.976146 13520 net.cpp:100] Creating Layer mnist
I0814 21:51:56.976155 13520 net.cpp:408] mnist -> data
I0814 21:51:56.976163 13520 net.cpp:408] mnist -> label
I0814 21:51:56.976171 13520 data_transformer.cpp:25] Loading mean file from: data/person_only_lmdb/person_background_only_color_mean.binaryproto
I0814 21:51:56.977579 13532 db_lmdb.cpp:35] Opened lmdb data/person_only_lmdb/person_background_only_test_lmdb
I0814 21:51:56.977890 13520 data_layer.cpp:41] output data size: 100,3,128,128
I0814 21:51:57.033671 13520 net.cpp:150] Setting up mnist
I0814 21:51:57.033711 13520 net.cpp:157] Top shape: 100 3 128 128 (4915200)
I0814 21:51:57.033720 13520 net.cpp:157] Top shape: 100 (100)
I0814 21:51:57.033725 13520 net.cpp:165] Memory required for data: 19661200
I0814 21:51:57.033735 13520 layer_factory.hpp:77] Creating layer label_mnist_1_split
I0814 21:51:57.033754 13520 net.cpp:100] Creating Layer label_mnist_1_split
I0814 21:51:57.033761 13520 net.cpp:434] label_mnist_1_split <- label
I0814 21:51:57.033771 13520 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_0
I0814 21:51:57.033788 13520 net.cpp:408] label_mnist_1_split -> label_mnist_1_split_1
I0814 21:51:57.034045 13520 net.cpp:150] Setting up label_mnist_1_split
I0814 21:51:57.034068 13520 net.cpp:157] Top shape: 100 (100)
I0814 21:51:57.034075 13520 net.cpp:157] Top shape: 100 (100)
I0814 21:51:57.034077 13520 net.cpp:165] Memory required for data: 19662000
I0814 21:51:57.034082 13520 layer_factory.hpp:77] Creating layer conv1
I0814 21:51:57.034101 13520 net.cpp:100] Creating Layer conv1
I0814 21:51:57.034106 13520 net.cpp:434] conv1 <- data
I0814 21:51:57.034114 13520 net.cpp:408] conv1 -> conv1
I0814 21:51:57.035461 13520 net.cpp:150] Setting up conv1
I0814 21:51:57.035483 13520 net.cpp:157] Top shape: 100 96 124 124 (147609600)
I0814 21:51:57.035488 13520 net.cpp:165] Memory required for data: 610100400
I0814 21:51:57.035503 13520 layer_factory.hpp:77] Creating layer pool1
I0814 21:51:57.035514 13520 net.cpp:100] Creating Layer pool1
I0814 21:51:57.035519 13520 net.cpp:434] pool1 <- conv1
I0814 21:51:57.035527 13520 net.cpp:408] pool1 -> pool1
I0814 21:51:57.037145 13520 net.cpp:150] Setting up pool1
I0814 21:51:57.037178 13520 net.cpp:157] Top shape: 100 96 62 62 (36902400)
I0814 21:51:57.037184 13520 net.cpp:165] Memory required for data: 757710000
I0814 21:51:57.037190 13520 layer_factory.hpp:77] Creating layer conv2
I0814 21:51:57.037214 13520 net.cpp:100] Creating Layer conv2
I0814 21:51:57.037221 13520 net.cpp:434] conv2 <- pool1
I0814 21:51:57.037235 13520 net.cpp:408] conv2 -> conv2
I0814 21:51:57.040050 13520 net.cpp:150] Setting up conv2
I0814 21:51:57.040071 13520 net.cpp:157] Top shape: 100 50 58 58 (16820000)
I0814 21:51:57.040076 13520 net.cpp:165] Memory required for data: 824990000
I0814 21:51:57.040088 13520 layer_factory.hpp:77] Creating layer pool2
I0814 21:51:57.040098 13520 net.cpp:100] Creating Layer pool2
I0814 21:51:57.040103 13520 net.cpp:434] pool2 <- conv2
I0814 21:51:57.040112 13520 net.cpp:408] pool2 -> pool2
I0814 21:51:57.040189 13520 net.cpp:150] Setting up pool2
I0814 21:51:57.040202 13520 net.cpp:157] Top shape: 100 50 29 29 (4205000)
I0814 21:51:57.040208 13520 net.cpp:165] Memory required for data: 841810000
I0814 21:51:57.040213 13520 layer_factory.hpp:77] Creating layer ip1
I0814 21:51:57.040226 13520 net.cpp:100] Creating Layer ip1
I0814 21:51:57.040235 13520 net.cpp:434] ip1 <- pool2
I0814 21:51:57.040246 13520 net.cpp:408] ip1 -> ip1
I0814 21:51:57.246063 13520 net.cpp:150] Setting up ip1
I0814 21:51:57.246119 13520 net.cpp:157] Top shape: 100 500 (50000)
I0814 21:51:57.246122 13520 net.cpp:165] Memory required for data: 842010000
I0814 21:51:57.246145 13520 layer_factory.hpp:77] Creating layer relu1
I0814 21:51:57.246182 13520 net.cpp:100] Creating Layer relu1
I0814 21:51:57.246189 13520 net.cpp:434] relu1 <- ip1
I0814 21:51:57.246197 13520 net.cpp:395] relu1 -> ip1 (in-place)
I0814 21:51:57.247073 13520 net.cpp:150] Setting up relu1
I0814 21:51:57.247088 13520 net.cpp:157] Top shape: 100 500 (50000)
I0814 21:51:57.247092 13520 net.cpp:165] Memory required for data: 842210000
I0814 21:51:57.247094 13520 layer_factory.hpp:77] Creating layer ip2
I0814 21:51:57.247120 13520 net.cpp:100] Creating Layer ip2
I0814 21:51:57.247125 13520 net.cpp:434] ip2 <- ip1
I0814 21:51:57.247133 13520 net.cpp:408] ip2 -> ip2
I0814 21:51:57.247293 13520 net.cpp:150] Setting up ip2
I0814 21:51:57.247301 13520 net.cpp:157] Top shape: 100 2 (200)
I0814 21:51:57.247304 13520 net.cpp:165] Memory required for data: 842210800
I0814 21:51:57.247310 13520 layer_factory.hpp:77] Creating layer ip2_ip2_0_split
I0814 21:51:57.247325 13520 net.cpp:100] Creating Layer ip2_ip2_0_split
I0814 21:51:57.247331 13520 net.cpp:434] ip2_ip2_0_split <- ip2
I0814 21:51:57.247337 13520 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_0
I0814 21:51:57.247345 13520 net.cpp:408] ip2_ip2_0_split -> ip2_ip2_0_split_1
I0814 21:51:57.247388 13520 net.cpp:150] Setting up ip2_ip2_0_split
I0814 21:51:57.247395 13520 net.cpp:157] Top shape: 100 2 (200)
I0814 21:51:57.247398 13520 net.cpp:157] Top shape: 100 2 (200)
I0814 21:51:57.247400 13520 net.cpp:165] Memory required for data: 842212400
I0814 21:51:57.247403 13520 layer_factory.hpp:77] Creating layer accuracy
I0814 21:51:57.247418 13520 net.cpp:100] Creating Layer accuracy
I0814 21:51:57.247424 13520 net.cpp:434] accuracy <- ip2_ip2_0_split_0
I0814 21:51:57.247428 13520 net.cpp:434] accuracy <- label_mnist_1_split_0
I0814 21:51:57.247436 13520 net.cpp:408] accuracy -> accuracy
I0814 21:51:57.247448 13520 net.cpp:150] Setting up accuracy
I0814 21:51:57.247453 13520 net.cpp:157] Top shape: (1)
I0814 21:51:57.247455 13520 net.cpp:165] Memory required for data: 842212404
I0814 21:51:57.247460 13520 layer_factory.hpp:77] Creating layer loss
I0814 21:51:57.247467 13520 net.cpp:100] Creating Layer loss
I0814 21:51:57.247470 13520 net.cpp:434] loss <- ip2_ip2_0_split_1
I0814 21:51:57.247474 13520 net.cpp:434] loss <- label_mnist_1_split_1
I0814 21:51:57.247480 13520 net.cpp:408] loss -> loss
I0814 21:51:57.247488 13520 layer_factory.hpp:77] Creating layer loss
I0814 21:51:57.247800 13520 net.cpp:150] Setting up loss
I0814 21:51:57.247810 13520 net.cpp:157] Top shape: (1)
I0814 21:51:57.247813 13520 net.cpp:160]     with loss weight 1
I0814 21:51:57.247825 13520 net.cpp:165] Memory required for data: 842212408
I0814 21:51:57.247828 13520 net.cpp:226] loss needs backward computation.
I0814 21:51:57.247834 13520 net.cpp:228] accuracy does not need backward computation.
I0814 21:51:57.247839 13520 net.cpp:226] ip2_ip2_0_split needs backward computation.
I0814 21:51:57.247843 13520 net.cpp:226] ip2 needs backward computation.
I0814 21:51:57.247846 13520 net.cpp:226] relu1 needs backward computation.
I0814 21:51:57.247849 13520 net.cpp:226] ip1 needs backward computation.
I0814 21:51:57.247853 13520 net.cpp:226] pool2 needs backward computation.
I0814 21:51:57.247858 13520 net.cpp:226] conv2 needs backward computation.
I0814 21:51:57.247861 13520 net.cpp:226] pool1 needs backward computation.
I0814 21:51:57.247864 13520 net.cpp:226] conv1 needs backward computation.
I0814 21:51:57.247869 13520 net.cpp:228] label_mnist_1_split does not need backward computation.
I0814 21:51:57.247874 13520 net.cpp:228] mnist does not need backward computation.
I0814 21:51:57.247875 13520 net.cpp:270] This network produces output accuracy
I0814 21:51:57.247879 13520 net.cpp:270] This network produces output loss
I0814 21:51:57.247894 13520 net.cpp:283] Network initialization done.
I0814 21:51:57.247972 13520 solver.cpp:60] Solver scaffolding done.
I0814 21:51:57.249361 13520 solver.cpp:337] Iteration 0, Testing net (#0)
I0814 21:51:59.578598 13520 blocking_queue.cpp:50] Data layer prefetch queue empty
I0814 21:52:03.012840 13520 solver.cpp:404]     Test net output #0: accuracy = 0.445465
I0814 21:52:03.012868 13520 solver.cpp:404]     Test net output #1: loss = 0.710531 (* 1 = 0.710531 loss)
I0814 21:52:03.048133 13520 solver.cpp:228] Iteration 0, loss = 0.68864
I0814 21:52:03.048172 13520 solver.cpp:244]     Train net output #0: loss = 0.68864 (* 1 = 0.68864 loss)
I0814 21:52:03.048182 13520 sgd_solver.cpp:106] Iteration 0, lr = 1e-05
I0814 21:52:12.059630 13520 solver.cpp:337] Iteration 100, Testing net (#0)
I0814 21:52:17.875958 13520 solver.cpp:404]     Test net output #0: accuracy = 0.577209
I0814 21:52:17.876013 13520 solver.cpp:404]     Test net output #1: loss = 0.637163 (* 1 = 0.637163 loss)
I0814 21:52:17.897013 13520 solver.cpp:228] Iteration 100, loss = 0.660059
I0814 21:52:17.897033 13520 solver.cpp:244]     Train net output #0: loss = 0.660059 (* 1 = 0.660059 loss)
I0814 21:52:17.897053 13520 sgd_solver.cpp:106] Iteration 100, lr = 9.96266e-06
I0814 21:52:26.943111 13520 solver.cpp:337] Iteration 200, Testing net (#0)
I0814 21:52:32.762687 13520 solver.cpp:404]     Test net output #0: accuracy = 0.589535
I0814 21:52:32.762753 13520 solver.cpp:404]     Test net output #1: loss = 0.604124 (* 1 = 0.604124 loss)
I0814 21:52:32.783748 13520 solver.cpp:228] Iteration 200, loss = 0.645822
I0814 21:52:32.783820 13520 solver.cpp:244]     Train net output #0: loss = 0.645822 (* 1 = 0.645822 loss)
I0814 21:52:32.783834 13520 sgd_solver.cpp:106] Iteration 200, lr = 9.92565e-06
I0814 21:52:41.833829 13520 solver.cpp:337] Iteration 300, Testing net (#0)
I0814 21:52:47.650267 13520 solver.cpp:404]     Test net output #0: accuracy = 0.596221
I0814 21:52:47.650327 13520 solver.cpp:404]     Test net output #1: loss = 0.588232 (* 1 = 0.588232 loss)
I0814 21:52:47.676832 13520 solver.cpp:228] Iteration 300, loss = 0.587351
I0814 21:52:47.676889 13520 solver.cpp:244]     Train net output #0: loss = 0.587351 (* 1 = 0.587351 loss)
I0814 21:52:47.676908 13520 sgd_solver.cpp:106] Iteration 300, lr = 9.88896e-06
I0814 21:52:56.757369 13520 solver.cpp:337] Iteration 400, Testing net (#0)
I0814 21:53:02.588917 13520 solver.cpp:404]     Test net output #0: accuracy = 0.631628
I0814 21:53:02.588963 13520 solver.cpp:404]     Test net output #1: loss = 0.570496 (* 1 = 0.570496 loss)
I0814 21:53:02.612507 13520 solver.cpp:228] Iteration 400, loss = 0.579072
I0814 21:53:02.612541 13520 solver.cpp:244]     Train net output #0: loss = 0.579072 (* 1 = 0.579072 loss)
I0814 21:53:02.612555 13520 sgd_solver.cpp:106] Iteration 400, lr = 9.85258e-06
